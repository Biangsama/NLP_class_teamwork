{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据集介绍\n",
    "\n",
    "<font size=4>[TREC 2006 Spam Track Public Corpora](https://plg.uwaterloo.ca/~gvcormac/treccorpus06/about.html)是一个公开的垃圾邮件语料库，由国际文本检索会议提供，分为英文数据集（trec06p）和中文数据集（trec06c），其中所含的邮件均来源于真实邮件保留了邮件的原有格式和内容。</font>\n",
    "\n",
    "<font size=4>除TREC 2006外，还有TREC 2005和TREC 2007的英文垃圾邮件数据集（对，没有中文），本项目中，仅使用TREC 200提供的中文数据集进行演示。TREC 2005-2007的垃圾邮件数据集，均已整理在项目挂载的数据集中，感兴趣的读者可以自行fork。</font>\n",
    "\n",
    "<font size=4>文件目录形式：delay和full分别是一种垃圾邮件过滤器的过滤机制，full目录下，是理想的邮件分类结果，我们可以视为研究的标签。</font>\n",
    "```\n",
    "trec06c\n",
    "│\n",
    "└───data\n",
    "│   │   000\n",
    "│   │   001\n",
    "│   │   ...\n",
    "│   └───215\n",
    "└───delay\n",
    "│   │   index\n",
    "└───full\n",
    "│   │   index  \n",
    "```\n",
    "\n",
    "邮件内容样本示例：\n",
    "```\n",
    "负责人您好我是深圳金海实业有限公司 广州 东莞 等省市有分公司我司有良好的社会关系和实力 因每月进项多出项少现有一部分发票可优惠对外代开税率较低 增值税发票为 其它国税 地税运输 广告等普通发票为 的税点 还可以根据数目大小来衡量优惠的多少 希望贵公司 商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话 小时服务信箱联系人 张海南顺祝商祺深圳市金海实业有限公司\n",
    "```\n",
    "```\n",
    "GG非常好的朋友H在计划马上的西藏自助游（完全靠搭车的那种），我和H也是很早认识的朋友，他有女朋友，在一起10年了，感情很好。\n",
    "GG对旅游兴趣不大。而且喜欢跟着旅行社的那种。所以肯定不去。\n",
    "我在没有认识GG前，时常独自去一些地方，从南到北，觉得旅行不应该目的那么强。\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 一、环境配置\n",
    "\n",
    "本项目基于Paddle 2.0 编写，如果你的环境不是本版本，请先参考官网[安装](https://www.paddlepaddle.org.cn/install/quick) Paddle 2.0 。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T07:50:23.639139Z",
     "start_time": "2024-10-17T07:50:19.393820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate\n",
      "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: click in d:\\nlp_email\\.venv\\lib\\site-packages (from translate) (8.1.7)\n",
      "Collecting lxml (from translate)\n",
      "  Downloading lxml-5.3.0-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: requests in d:\\nlp_email\\.venv\\lib\\site-packages (from translate) (2.32.3)\n",
      "Collecting libretranslatepy==2.1.1 (from translate)\n",
      "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
      "Requirement already satisfied: colorama in d:\\nlp_email\\.venv\\lib\\site-packages (from click->translate) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\nlp_email\\.venv\\lib\\site-packages (from requests->translate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nlp_email\\.venv\\lib\\site-packages (from requests->translate) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\nlp_email\\.venv\\lib\\site-packages (from requests->translate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlp_email\\.venv\\lib\\site-packages (from requests->translate) (2024.7.4)\n",
      "Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
      "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.8/3.8 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: libretranslatepy, lxml, translate\n",
      "Successfully installed libretranslatepy-2.1.1 lxml-5.3.0 translate-3.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T07:50:26.075435Z",
     "start_time": "2024-10-17T07:50:23.641141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你会说英文吗\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from translate import Translator\n",
    "#在任何两种语言之间，中文翻译成英文\n",
    "translator=Translator(from_lang=\"english\",to_lang=\"chinese\")\n",
    "translation = translator.translate(\"can you speak english?\")\n",
    "print(translation)\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translator = Translator(to_lang=\"zh\")\n",
    "        translated_text = translator.translate(text)\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:22:54.293231Z",
     "start_time": "2024-10-17T09:22:47.792719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OMP_NUM_THREADS set to 12, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n",
      "D:\\NLP_email\\.venv\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# 导入相关的模块\n",
    "import re\n",
    "import jieba\n",
    "import os \n",
    "import random\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from visualdl import LogWriter\n",
    "import numpy as np\n",
    "from functools import partial #partial()函数可以用来固定某些参数值，并返回一个新的callable对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:22:54.309235Z",
     "start_time": "2024-10-17T09:22:54.295232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.2\n"
     ]
    }
   ],
   "source": [
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据加载\n",
    "\n",
    "## 2.1 数据集准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T07:58:58.851389Z",
     "start_time": "2024-10-17T07:58:58.614336Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'data/data89631/trec06c.tgz'\n"
     ]
    }
   ],
   "source": [
    "# 解压数据集\n",
    "!tar xvf data/data89631/trec06c.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 提取邮件内容，划分训练集、验证集、测试集\n",
    "<font size=4 color=red>本项目中，截取中文邮件提取内容的最后200个字符作为文本分类任务的输入，但是这里需要特别注意的是，个别邮件提取结果为`null`，在BERT预训练模型的finetune任务中，如果输入为空会产生报错。</font>\n",
    "\n",
    "<font size=4>因此，在生成训练集、验证集、测试集前，要进行数据清洗。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:05:22.238478Z",
     "start_time": "2024-10-17T09:05:22.238478Z"
    }
   },
   "outputs": [],
   "source": [
    "# 去掉非中文字符\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "# 从指定路径读取邮件文件内容信息\n",
    "def get_data_in_a_file(original_path, save_path='all_email.txt'):\n",
    "    email = ''\n",
    "    f = open(original_path, 'r', encoding='gb2312', errors='ignore')\n",
    "    # lines = f.readlines()\n",
    "    for line in f:\n",
    "        # 去掉换行符\n",
    "        line = line.strip().strip('\\n')\n",
    "        # 去掉非中文字符\n",
    "        line = clean_str(line)\n",
    "        email += line\n",
    "    f.close()\n",
    "    # 只保留末尾200个字符\n",
    "    return email[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:03:48.801433Z",
     "start_time": "2024-10-17T08:03:48.787430Z"
    }
   },
   "outputs": [],
   "source": [
    "f1 = open('train_list2.txt', 'r')\n",
    "translator=Translator(from_lang=\"english\",to_lang=\"chinese\")\n",
    "i=0\n",
    "for line in f1:\n",
    "    line=line.strip()\n",
    "    translation = translate_text(line[:-2])\n",
    "    if translation==None:\n",
    "        continue\n",
    "    i+=1\n",
    "    print(i)\n",
    "    if i==50:\n",
    "        break\n",
    "    label=line[-1]\n",
    "    translation = clean_str(translation)\n",
    "    translation=translation.replace(\" \",\",\")\n",
    "    # 设置垃圾邮件的标签为0\n",
    "    with open(\"train_list3.txt\",\"a+\") as f2:\n",
    "                    f2.write(translation + '\\t' + label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:06:36.374738Z",
     "start_time": "2024-10-17T08:05:58.316826Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取标签文件信息\n",
    "f = open('trec06c/full/index', 'r')\n",
    "for line in f:\n",
    "    str_list = line.split(\" \")\n",
    "    # 设置垃圾邮件的标签为0\n",
    "    if str_list[0] == 'spam':\n",
    "        label = '0'\n",
    "    # 设置正常邮件标签为1\n",
    "    elif str_list[0] == 'ham':\n",
    "        label = '1'\n",
    "    text = get_data_in_a_file('trec06c/full/' + str(str_list[1].split(\"\\n\")[0]))\n",
    "    with open(\"all_email.txt\",\"a+\",encoding='utf-8') as f:\n",
    "                    f.write(text + '\\t' + label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:01.755088Z",
     "start_time": "2024-10-17T09:23:01.360999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据列表生成完成！\n"
     ]
    }
   ],
   "source": [
    "data_list_path=\"./\"\n",
    "\n",
    "with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\n",
    "    f_eval.seek(0)\n",
    "    f_eval.truncate()\n",
    "    \n",
    "with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\n",
    "    f_train.seek(0)\n",
    "    f_train.truncate() \n",
    "\n",
    "with open(os.path.join(data_list_path, 'test_list.txt'), 'w', encoding='utf-8') as f_test:\n",
    "    f_test.seek(0)\n",
    "    f_test.truncate()\n",
    "\n",
    "with open(os.path.join(data_list_path, 'all_email.txt'), 'r', encoding='utf-8') as f_data:\n",
    "    lines = f_data.readlines()\n",
    "\n",
    "i = 0\n",
    "with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'test_list.txt'), 'a', encoding='utf-8') as f_test,open(os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "    for line in lines:\n",
    "        # 提取label信息\n",
    "        label = line.split('\\t')[-1].replace('\\n', '')\n",
    "        # 提取输入文本信息\n",
    "        words = line.split('\\t')[0]\n",
    "        # 邮件文本提取结果中有大量空格，这里统一用逗号替换\n",
    "        words = words.replace(' ', ',')\n",
    "        labs = \"\"\n",
    "        # 数据清洗，如果输入文本内容为空，则视为脏数据予以提出，避免在BERT模型finetune时报错\n",
    "        if len(words) > 0:\n",
    "            # 划分验证集\n",
    "            if i % 10 == 1:\n",
    "                labs = words + '\\t' + label + '\\n'\n",
    "                f_eval.write(labs)\n",
    "            # 划分测试集\n",
    "            elif i % 10 == 2:\n",
    "                labs = words + '\\t' + label + '\\n'\n",
    "                f_test.write(labs)\n",
    "            # 划分训练集\n",
    "            else:\n",
    "                labs = words + '\\t' + label + '\\n'\n",
    "                f_train.write(labs)\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "print(\"数据列表生成完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 自定义数据集\n",
    "在示例项目中，BERT模型finetune的数据集为公开中文情感分析数据集ChnSenticorp。使用PaddleNLP的.datasets.ChnSentiCorp.get_datasets方法即可以加载该数据集。\n",
    "\n",
    "<font size=4 color=red>在本项目中，我们需要自定义数据集，并使自定义数据集后的数据格式与使用`ppnlp.datasets.ChnSentiCorp.get_datasets(['train','dev','test'])\n",
    "`加载后完全一致。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:04.513733Z",
     "start_time": "2024-10-17T09:23:04.365698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "训练集数据：[['公司艾克森金山石化,中国化工进出口公司,正大集团大福饲料,厦华集团,灿坤股份东金电子,太原钢铁集团,深圳开发科技,大冷王运输制冷,三洋华强,等知名企业提供项目辅导或专题培训,王老师授课狙榉岣唬风格幽默诙谐,逻辑清晰,过程互动,案例生动,深受学员喜爱授课时间,地点月,周六,日,上海课,程,费,用元,人,包含培训费用,午餐,证书,资料优惠,三人以上参加,赠送一名额联,系,我,们联系人,桂先生电话,传真', '0'], ['贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', '0'], ['用付出劳动,那就交注册费吧,呵呵,让网站去赚你注册费的,吧,你注册费的,付给你的上线,那样你真的赚到什么了吗,真搞不懂当您发展下线时,只需将本页的注册连接中的,换成您在,的用户名即可独乐乐,不如众乐乐,大家一起赚美国人的钱吧把这个连接,全部蓝色部份,复制到浏览器地址栏中,回车即可进入注册界面我的邮件地址广告,网络电话包年卡,元,长途市话全包最快的论坛邮址搜索专家,最好的邮件群发专家论坛短信群发专家', '0']]\n",
      "\n",
      "验证集数据:[['讲的是孔子后人的故事,一个老领导回到家乡,跟儿子感情不和,跟贪财的孙子孔为本和睦老领导的弟弟魏宗万是赶马车的有个洋妞大概是考察民俗的,在他们家过年孔为本总想出国,被爷爷教育了最后,一家人基本和解顺便问另一类电影,北京青年电影制片厂的,中越战背景,一军人被介绍了一个对象,去相亲,女方是军队医院的护士,犹豫不决,总是在回忆战场上负伤的男友,好像还没死,最后男方表示理解,归队了', '1'], ['贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', '0'], ['可以代理代办其它发票如,广告,运输,建筑其它服务行业都可以代理代办,我公司因全年为外商代理进出口业务,所开的税额用海关缴款书在当地税务部门已抵税,等于我司纳税后才开出,正常我司的税收点数比较低,请各公司放心我公司都有正当手续,如有希要以上业务的公司,厂家,请向我司主管人员联系本公司向所有公司,厂家,承诺先验票后付款,真诚期待与贵公司,厂家,合作欢迎来电咨询深圳协恒实业有限公司联系人,张永辉联系电话', '0']]\n",
      "\n",
      "测试集数据:[[',负责人您好我是深圳金海实业有限公司,广州,东莞,等省市有分公司我司有良好的社会关系和实力,因每月进项多出项少现有一部分发票可优惠对外代开税率较低,增值税发票为,其它国税,地税运输,广告等普通发票为,的税点,还可以根据数目大小来衡量优惠的多少,希望贵公司,商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话,小时服务信箱联系人,张海南顺祝商祺深圳市金海实业有限公司', '0'], ['您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺', '0'], [',负责人您好我是深圳联美实业有限公司,广州,东莞,等省市有分公司我司有良好的社会关系和实力,因每月进项多出项少现有一部分发票可优惠对外代开税率较低,增值税发票为,其它国税,地税运输,广告等普通发票为,的税点,还可以根据数目大小来衡量优惠的多少,希望贵公司,商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话,小时服务信箱联系人,郭江河顺祝商祺深圳市联美实业有限公司', '0']]\n"
     ]
    }
   ],
   "source": [
    "class SelfDefinedDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(SelfDefinedDataset, self).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "def txt_to_list(file_name):\n",
    "    res_list = []\n",
    "    for line in open(file_name,encoding='utf-8'):\n",
    "        res_list.append(line.strip().split('\\t'))\n",
    "    return res_list\n",
    "\n",
    "trainlst = txt_to_list('train_list.txt')\n",
    "devlst = txt_to_list('eval_list.txt')\n",
    "testlst = txt_to_list('test_list.txt')\n",
    "\n",
    "train_ds = SelfDefinedDataset(trainlst)\n",
    "dev_ds = SelfDefinedDataset(devlst)\n",
    "test_ds = SelfDefinedDataset(testlst)\n",
    "label_list = train_ds.get_labels()\n",
    "print(label_list)\n",
    "from paddlenlp.datasets import MapDataset\n",
    "train_ds = MapDataset(train_ds)\n",
    "dev_ds = MapDataset(dev_ds)\n",
    "test_ds = MapDataset(test_ds)\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:3]))\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:3]))\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:07.334336Z",
     "start_time": "2024-10-17T09:23:07.313293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据：[['公司艾克森金山石化,中国化工进出口公司,正大集团大福饲料,厦华集团,灿坤股份东金电子,太原钢铁集团,深圳开发科技,大冷王运输制冷,三洋华强,等知名企业提供项目辅导或专题培训,王老师授课狙榉岣唬风格幽默诙谐,逻辑清晰,过程互动,案例生动,深受学员喜爱授课时间,地点月,周六,日,上海课,程,费,用元,人,包含培训费用,午餐,证书,资料优惠,三人以上参加,赠送一名额联,系,我,们联系人,桂先生电话,传真', '0'], ['贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', '0'], ['用付出劳动,那就交注册费吧,呵呵,让网站去赚你注册费的,吧,你注册费的,付给你的上线,那样你真的赚到什么了吗,真搞不懂当您发展下线时,只需将本页的注册连接中的,换成您在,的用户名即可独乐乐,不如众乐乐,大家一起赚美国人的钱吧把这个连接,全部蓝色部份,复制到浏览器地址栏中,回车即可进入注册界面我的邮件地址广告,网络电话包年卡,元,长途市话全包最快的论坛邮址搜索专家,最好的邮件群发专家论坛短信群发专家', '0']]\n",
      "\n",
      "验证集数据:[['讲的是孔子后人的故事,一个老领导回到家乡,跟儿子感情不和,跟贪财的孙子孔为本和睦老领导的弟弟魏宗万是赶马车的有个洋妞大概是考察民俗的,在他们家过年孔为本总想出国,被爷爷教育了最后,一家人基本和解顺便问另一类电影,北京青年电影制片厂的,中越战背景,一军人被介绍了一个对象,去相亲,女方是军队医院的护士,犹豫不决,总是在回忆战场上负伤的男友,好像还没死,最后男方表示理解,归队了', '1'], ['贵公司负责人,经理,财务,您好深圳市华龙公司受多家公司委托向外低点代开部分增值税电脑发票,左右,和普通商品销售税发票,国税,地税运输,广告,服务等票,左右,还可以根据所做数量额度的大小来商讨优惠的点数本公司郑重承诺所用绝对是真票,可验证后付款此信息长期有效,如须进一步洽商请电联系人,刘剑辉顺祝商祺低点代开发票', '0'], ['可以代理代办其它发票如,广告,运输,建筑其它服务行业都可以代理代办,我公司因全年为外商代理进出口业务,所开的税额用海关缴款书在当地税务部门已抵税,等于我司纳税后才开出,正常我司的税收点数比较低,请各公司放心我公司都有正当手续,如有希要以上业务的公司,厂家,请向我司主管人员联系本公司向所有公司,厂家,承诺先验票后付款,真诚期待与贵公司,厂家,合作欢迎来电咨询深圳协恒实业有限公司联系人,张永辉联系电话', '0']]\n",
      "\n",
      "测试集数据:[[',负责人您好我是深圳金海实业有限公司,广州,东莞,等省市有分公司我司有良好的社会关系和实力,因每月进项多出项少现有一部分发票可优惠对外代开税率较低,增值税发票为,其它国税,地税运输,广告等普通发票为,的税点,还可以根据数目大小来衡量优惠的多少,希望贵公司,商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话,小时服务信箱联系人,张海南顺祝商祺深圳市金海实业有限公司', '0'], ['您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺', '0'], [',负责人您好我是深圳联美实业有限公司,广州,东莞,等省市有分公司我司有良好的社会关系和实力,因每月进项多出项少现有一部分发票可优惠对外代开税率较低,增值税发票为,其它国税,地税运输,广告等普通发票为,的税点,还可以根据数目大小来衡量优惠的多少,希望贵公司,商家等来电商谈欢迎合作本公司郑重承诺所用票据可到税务局验证或抵扣欢迎来电进一步商谈电话,小时服务信箱联系人,郭江河顺祝商祺深圳市联美实业有限公司', '0']]\n",
      "\n",
      "训练集样本个数:49510\n",
      "验证集样本个数:6189\n",
      "测试集样本个数:6189\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#看看数据长什么样子，分别打印训练集、验证集、测试集的前3条数据。\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:3]))\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:3]))\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:3]))\n",
    "\n",
    "print(\"训练集样本个数:{}\".format(len(train_ds)))\n",
    "print(\"验证集样本个数:{}\".format(len(dev_ds)))\n",
    "print(\"测试集样本个数:{}\".format(len(test_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:11.250828Z",
     "start_time": "2024-10-17T09:23:11.220821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2024-10-17 17:23:11,227] [    INFO]\u001B[0m - Already cached C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\bert-base-chinese-vocab.txt\u001B[0m\n",
      "\u001B[32m[2024-10-17 17:23:11,238] [    INFO]\u001B[0m - tokenizer config file saved in C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\tokenizer_config.json\u001B[0m\n",
      "\u001B[32m[2024-10-17 17:23:11,240] [    INFO]\u001B[0m - Special tokens file saved in C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\special_tokens_map.json\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#调用ppnlp.transformers.BertTokenizer进行数据处理，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "#数据预处理\n",
    "def convert_example(example,tokenizer,label_list,max_seq_length=256,is_test=False):\n",
    "    if is_test:\n",
    "        text = example\n",
    "    else:\n",
    "        text, label = example\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    #注意，在早前的PaddleNLP版本中，token_type_ids叫做segment_ids\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label_map = {}\n",
    "        for (i, l) in enumerate(label_list):\n",
    "            label_map[l] = i\n",
    "\n",
    "        label = label_map[label]\n",
    "        label = np.array([label], dtype=\"int64\")\n",
    "        return input_ids, segment_ids, label\n",
    "    else:\n",
    "        return input_ids, segment_ids\n",
    "\n",
    "#数据迭代器构造方法\n",
    "def create_dataloader(dataset, trans_fn=None, mode='train', batch_size=1, use_gpu=True, pad_token_id=0, batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn, lazy=True)\n",
    "\n",
    "    if mode == 'train' and use_gpu:\n",
    "        sampler = paddle.io.DistributedBatchSampler(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        shuffle = True if mode == 'train' else False #如果不是训练集，则不打乱顺序\n",
    "        sampler = paddle.io.BatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle) #生成一个取样器\n",
    "    dataloader = paddle.io.DataLoader(dataset, batch_sampler=sampler, return_list=True, collate_fn=batchify_fn)\n",
    "    return dataloader\n",
    "\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_list, max_seq_length, is_test等参数值\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, label_list=label_list, max_seq_length=128, is_test=False)\n",
    "batchify_fn = lambda samples, fn=Tuple(Pad(axis=0,pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id), Stack(dtype=\"int64\")):[data for data in fn(samples)]\n",
    "#训练集迭代器\n",
    "train_loader = create_dataloader(train_ds, mode='train', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "#验证集迭代器\n",
    "dev_loader = create_dataloader(dev_ds, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "#测试集迭代器\n",
    "test_loader = create_dataloader(test_ds, mode='test', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#devlist2=txt_to_list('train_list3.txt')\n",
    "#dev_ds2=SelfDefinedDataset(devlist2)\n",
    "#dev_ds2=MapDataset(dev_ds2)\n",
    "#dev_loader2 = create_dataloader(dev_ds2, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型训练\n",
    "## 3.1 加载BERT预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:25.511262Z",
     "start_time": "2024-10-17T09:23:15.110075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2024-10-17 17:23:15,115] [    INFO]\u001B[0m - Already cached C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 17:23:15,116] [    INFO]\u001B[0m - Loading weights file model_state.pdparams from cache at C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 17:23:15,704] [    INFO]\u001B[0m - Loaded weights file from disk, setting weights to model.\u001B[0m\n",
      "\u001B[33m[2024-10-17 17:23:25,476] [ WARNING]\u001B[0m - Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001B[0m\n",
      "\u001B[33m[2024-10-17 17:23:25,477] [ WARNING]\u001B[0m - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#加载预训练模型Bert用于文本分类任务的Fine-tune网络BertForSequenceClassification, 它在BERT模型后接了一个全连接层进行分类。\n",
    "#由于本任务中的垃圾邮件识别是二分类问题，设定num_classes为2\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/, https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib) (2019.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (56.2.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib seaborn scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:16:33.428315Z",
     "start_time": "2024-10-17T09:16:33.408311Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.transformers import BertForSequenceClassification\n",
    "from visualdl import LogWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 开始训练\n",
    "<font size=4>为了看到训练过程，这里需要引入VisualDL记录训练`log`信息。添加的方式如下：</font>\n",
    "```python\n",
    "from visualdl import LogWriter\n",
    "with LogWriter(logdir=\"./log\") as writer:\n",
    "    writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "    writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "    writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\n",
    "    writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)\n",
    "```\n",
    "<font size=4>具体实现详见下方代码。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:25.526327Z",
     "start_time": "2024-10-17T09:23:25.513263Z"
    }
   },
   "outputs": [],
   "source": [
    "#设置训练超参数\n",
    "\n",
    "#学习率\n",
    "learning_rate = 1e-6 \n",
    "#训练轮次\n",
    "epochs = 3\n",
    "#学习率预热比率\n",
    "warmup_proption = 0.1\n",
    "#权重衰减系数\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "num_warmup_steps = int(warmup_proption * num_training_steps)\n",
    "\n",
    "def get_lr_factor(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    else:\n",
    "        return max(0.0,\n",
    "                    float(num_training_steps - current_step) /\n",
    "                    float(max(1, num_training_steps - num_warmup_steps)))\n",
    "#学习率调度器\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\n",
    "\n",
    "#优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "#损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "#评估函数\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:23:25.542269Z",
     "start_time": "2024-10-17T09:23:25.528267Z"
    }
   },
   "outputs": [],
   "source": [
    "#评估函数，设置返回值，便于VisualDL记录\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T09:26:01.143820Z",
     "start_time": "2024-10-17T09:23:25.544270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "D:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 1\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 5470, 1184, ..., 117 , 704 , 102 ],\n",
      "        [101 , 762 , 1874, ..., 4638, 2130, 102 ],\n",
      "        [101 , 1912, 2845, ..., 1071, 2124, 102 ],\n",
      "        ...,\n",
      "        [101 , 4500, 4638, ..., 4638, 798 , 102 ],\n",
      "        [101 , 5106, 3796, ..., 117 , 1555, 102 ],\n",
      "        [101 , 2245, 1184, ..., 117 , 2769, 102 ]])\n",
      "logits: Tensor(shape=[64, 2], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.20209090, -0.39877081],\n",
      "        [ 0.74738932, -0.73671073],\n",
      "        [ 0.78613943, -0.59866434],\n",
      "        [ 0.77173334, -0.18493524],\n",
      "        [ 0.52312410, -0.38009256],\n",
      "        [ 0.91436362, -0.06754762],\n",
      "        [ 0.58960402, -0.68650067],\n",
      "        [ 0.16917586, -0.26603153],\n",
      "        [ 0.53388309, -0.63708985],\n",
      "        [ 0.65927619, -0.55190331],\n",
      "        [ 0.63688868, -0.80141580],\n",
      "        [ 0.27530465, -0.15779850],\n",
      "        [ 0.54590845, -0.77733517],\n",
      "        [ 0.55554712, -0.73744410],\n",
      "        [ 0.22147334, -0.26655912],\n",
      "        [ 0.53456819, -0.37697524],\n",
      "        [ 0.58162045, -0.32790440],\n",
      "        [ 0.32365099, -0.19103904],\n",
      "        [ 0.70382261, -0.66542542],\n",
      "        [ 0.24188924, -0.10711402],\n",
      "        [ 0.62121522, -0.50385493],\n",
      "        [ 0.85637087, -0.45043528],\n",
      "        [ 0.06559223, -0.23623586],\n",
      "        [ 0.64467180, -0.11007661],\n",
      "        [ 0.39017904, -0.44772357],\n",
      "        [ 0.30001271, -0.65149945],\n",
      "        [ 0.76045871, -0.70200646],\n",
      "        [ 0.31703854, -0.08514132],\n",
      "        [ 0.52435446, -0.56384230],\n",
      "        [ 0.03863060, -0.03158182],\n",
      "        [ 0.58722913, -0.35710603],\n",
      "        [ 0.54447198, -0.41610599],\n",
      "        [ 0.55746770, -0.54101336],\n",
      "        [ 0.40639451, -0.24790034],\n",
      "        [ 0.25128347, -0.74302757],\n",
      "        [ 0.40567851, -0.32042497],\n",
      "        [ 0.23139474, -0.52909386],\n",
      "        [ 0.60057062, -0.38821888],\n",
      "        [ 0.31990245, -0.37347472],\n",
      "        [ 0.56665373,  0.13554947],\n",
      "        [ 0.46691722, -0.40976262],\n",
      "        [ 0.60788947, -0.59384769],\n",
      "        [ 0.38159239, -0.39726928],\n",
      "        [ 0.68732101, -0.82742018],\n",
      "        [ 0.02007312, -0.31008095],\n",
      "        [ 0.47995350, -0.49291965],\n",
      "        [-0.00942650,  0.15376157],\n",
      "        [ 0.26045009, -0.40550596],\n",
      "        [ 0.43342522, -0.59121108],\n",
      "        [ 0.72457123, -1.01982594],\n",
      "        [ 0.45235935, -0.41102844],\n",
      "        [ 0.37630868, -0.25025159],\n",
      "        [ 0.68389940, -0.36878181],\n",
      "        [ 0.48372114, -0.72046196],\n",
      "        [ 0.26420987, -0.20812406],\n",
      "        [ 0.36112756, -0.69048923],\n",
      "        [ 0.34929439, -0.43070278],\n",
      "        [ 0.06192140, -0.16264604],\n",
      "        [ 0.36390498, -0.42517585],\n",
      "        [ 0.41260493, -0.48146746],\n",
      "        [ 0.44373494, -0.42690074],\n",
      "        [ 0.59083343, -0.49797758],\n",
      "        [ 0.54714566, -0.47601941],\n",
      "        [ 0.08070631,  0.13854955]])\n",
      "loss: Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       0.56177849)\n",
      "acc: 0.734375\n",
      "global_step: 1\n",
      "step: 2\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 3119, 5592, ..., 1798, 4638, 102 ],\n",
      "        [101 , 5052, 800 , ..., 711 , 749 , 102 ],\n",
      "        [101 , 1435, 117 , ..., 677 , 4415, 102 ],\n",
      "        ...,\n",
      "        [101 , 5334, 2094, ..., 3341, 6656, 102 ],\n",
      "        [101 , 117 , 3680, ..., 4638, 4696, 102 ],\n",
      "        [101 , 2769, 1762, ..., 720 , 719 , 102 ]])\n",
      "logits: Tensor(shape=[64, 2], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.43735301, -0.22620767],\n",
      "        [ 0.78172445, -0.42299080],\n",
      "        [ 0.04849847, -0.03874174],\n",
      "        [ 0.55364823, -0.65559536],\n",
      "        [ 0.44819283, -0.38563851],\n",
      "        [ 0.51210403, -0.82737744],\n",
      "        [ 0.37316203, -0.26372498],\n",
      "        [ 0.21262369, -0.38957095],\n",
      "        [ 0.03169931, -0.01210636],\n",
      "        [ 0.22698829, -0.09518123],\n",
      "        [ 0.76128888, -0.21364667],\n",
      "        [ 0.56374943, -0.16230732],\n",
      "        [ 0.57691449, -0.33410150],\n",
      "        [ 0.08350046, -0.37176073],\n",
      "        [ 0.42979923, -0.37286282],\n",
      "        [ 0.33770716, -0.41094357],\n",
      "        [ 0.53136146, -0.31351119],\n",
      "        [ 0.32107866, -0.78781843],\n",
      "        [ 0.63057077, -0.77908015],\n",
      "        [ 0.40838122, -0.38872916],\n",
      "        [ 0.47178346, -0.97309554],\n",
      "        [ 0.51161301, -0.46976429],\n",
      "        [ 0.31587839, -0.25248146],\n",
      "        [ 0.50248146, -0.74534065],\n",
      "        [ 0.47441638, -0.59437442],\n",
      "        [ 0.10350317, -0.58013576],\n",
      "        [ 0.39902869,  0.31819463],\n",
      "        [ 0.28576726, -0.07903074],\n",
      "        [ 0.31339309,  0.02391508],\n",
      "        [ 0.47781134, -0.13064040],\n",
      "        [ 0.36347714, -0.39586917],\n",
      "        [ 0.84512264, -0.66310155],\n",
      "        [ 0.40246761, -0.28493533],\n",
      "        [ 0.71693063, -0.35382450],\n",
      "        [ 0.52907151, -0.13901445],\n",
      "        [ 0.73473048, -0.27197844],\n",
      "        [ 0.54725075, -0.39792934],\n",
      "        [ 0.82024705, -0.75604808],\n",
      "        [ 0.06453256, -0.31930959],\n",
      "        [ 0.63439304, -0.23152637],\n",
      "        [ 0.48974872, -0.42173201],\n",
      "        [ 0.03192893, -0.06196799],\n",
      "        [ 0.21337891, -0.40383235],\n",
      "        [ 0.14058456, -0.34220630],\n",
      "        [ 0.78791964, -0.23964539],\n",
      "        [ 0.62205100, -0.61871606],\n",
      "        [ 0.28663805, -0.73817706],\n",
      "        [ 0.35290420, -0.51626945],\n",
      "        [ 0.26348430,  0.13607317],\n",
      "        [ 0.24721736, -0.25821990],\n",
      "        [ 0.13465379, -0.52655327],\n",
      "        [ 0.33404949, -0.58832532],\n",
      "        [ 0.52363729, -0.79492688],\n",
      "        [ 0.39107102, -0.66372985],\n",
      "        [ 0.65940285, -0.22789706],\n",
      "        [ 1.00054193, -0.54591906],\n",
      "        [ 0.21278200, -0.54771197],\n",
      "        [ 0.51171005, -0.90792406],\n",
      "        [ 0.72162503, -0.59583497],\n",
      "        [ 0.67341846, -0.50448632],\n",
      "        [ 0.20311159, -0.47549346],\n",
      "        [ 0.54334390, -0.16363671],\n",
      "        [ 0.56201798, -0.75101966],\n",
      "        [ 0.49464315, -0.41723007]])\n",
      "loss: Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       0.75216675)\n",
      "acc: 0.625\n",
      "global_step: 2\n",
      "step: 3\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 4157, 3144, ..., 117 , 945 , 102 ],\n",
      "        [101 , 2990, 2768, ..., 738 , 3300, 102 ],\n",
      "        [101 , 1766, 117 , ..., 2458, 4638, 102 ],\n",
      "        ...,\n",
      "        [101 , 1239, 4510, ..., 934 , 2339, 102 ],\n",
      "        [101 , 2644, 1962, ..., 0   , 0   , 0   ],\n",
      "        [101 , 5314, 833 , ..., 2399, 117 , 102 ]])\n",
      "logits: Tensor(shape=[64, 2], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.70939553, -0.89657795],\n",
      "        [-0.05550562, -0.67201364],\n",
      "        [ 0.34039125, -0.88929284],\n",
      "        [ 0.53551787, -0.54331160],\n",
      "        [ 0.93374383, -0.54287517],\n",
      "        [ 0.82821018, -0.40886387],\n",
      "        [ 0.18396524, -0.53772032],\n",
      "        [ 0.84633511, -0.57074153],\n",
      "        [ 0.62133616, -0.62335533],\n",
      "        [ 0.28225756, -0.25823161],\n",
      "        [ 0.38851756, -0.54381859],\n",
      "        [ 0.51736534, -0.25908124],\n",
      "        [ 0.62589586, -0.35303828],\n",
      "        [ 0.34550452, -0.32869145],\n",
      "        [ 0.28448889, -0.68207163],\n",
      "        [ 0.64056146, -0.86044061],\n",
      "        [ 0.29502335, -0.20841020],\n",
      "        [ 0.18833926, -0.35060006],\n",
      "        [ 0.61000264, -0.74897075],\n",
      "        [ 0.66930223, -0.40516114],\n",
      "        [ 0.70484585, -0.38010687],\n",
      "        [ 0.61535388, -0.52313733],\n",
      "        [ 0.44193107, -0.52116776],\n",
      "        [ 0.39971268, -0.37565914],\n",
      "        [ 0.78733838, -0.42961651],\n",
      "        [ 0.65794820, -0.86570919],\n",
      "        [ 0.32520139, -0.51609433],\n",
      "        [ 0.42168629, -0.31828898],\n",
      "        [ 0.28222573, -0.50115466],\n",
      "        [ 0.55669808,  0.01191454],\n",
      "        [ 0.48626870, -0.47230664],\n",
      "        [ 0.52787536, -0.54843354],\n",
      "        [ 0.36668384, -0.16342327],\n",
      "        [ 0.49573720, -0.61095804],\n",
      "        [ 0.36825272,  0.37705809],\n",
      "        [ 0.37378502, -0.70796835],\n",
      "        [ 0.47508430, -0.49315745],\n",
      "        [ 0.34032285, -0.42071691],\n",
      "        [ 0.58613241, -0.17387792],\n",
      "        [ 0.41301084, -0.43986809],\n",
      "        [ 0.60666615, -0.19153050],\n",
      "        [ 0.27831072, -0.10388752],\n",
      "        [ 0.81921589, -0.75427890],\n",
      "        [ 0.56897479, -0.46740234],\n",
      "        [ 0.32181168,  0.24024463],\n",
      "        [ 0.43397826, -0.77927488],\n",
      "        [ 0.41677380, -0.16496195],\n",
      "        [ 0.26153639, -0.16921106],\n",
      "        [ 0.36890915, -0.52066427],\n",
      "        [ 0.57631266, -0.59695548],\n",
      "        [ 0.66920733, -0.62339580],\n",
      "        [ 0.41533530,  0.18047819],\n",
      "        [ 0.49307775, -0.03712199],\n",
      "        [ 0.61404830, -0.37033254],\n",
      "        [ 0.73947370, -0.68344760],\n",
      "        [ 0.34960160, -0.28516638],\n",
      "        [ 0.46931079, -0.15336522],\n",
      "        [ 0.86783856,  0.15084848],\n",
      "        [ 0.32975119, -0.15797001],\n",
      "        [ 0.32047918, -0.46472174],\n",
      "        [ 0.57419860, -0.11434917],\n",
      "        [ 0.23965143, -0.59996581],\n",
      "        [ 0.32717288, -0.03404354],\n",
      "        [ 0.42890632, -0.63382971]])\n",
      "loss: Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       0.55291981)\n",
      "acc: 0.6458333333333334\n",
      "global_step: 3\n",
      "step: 4\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 812 , 2157, ..., 3300, 117 , 102 ],\n",
      "        [101 , 3187, 7444, ..., 0   , 0   , 0   ],\n",
      "        [101 , 689 , 117 , ..., 6437, 2792, 102 ],\n",
      "        ...,\n",
      "        [101 , 4867, 2644, ..., 0   , 0   , 0   ],\n",
      "        [101 , 3144, 7030, ..., 6395, 2772, 102 ],\n",
      "        [101 , 1377, 809 , ..., 2772, 1168, 102 ]])\n",
      "logits: Tensor(shape=[64, 2], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.38387677, -0.27798474],\n",
      "        [ 0.58907861, -0.69873166],\n",
      "        [ 0.31472033, -0.50323594],\n",
      "        [ 0.34990716, -0.12119026],\n",
      "        [ 0.69299674, -0.55494285],\n",
      "        [ 0.23433837, -0.09079089],\n",
      "        [ 0.20437874, -0.51293635],\n",
      "        [ 0.38915122, -0.05869586],\n",
      "        [ 0.44106722, -0.54207438],\n",
      "        [ 0.58332956, -0.56489104],\n",
      "        [ 0.60724080, -0.37804717],\n",
      "        [ 0.81992579, -0.48273656],\n",
      "        [ 0.59261382, -0.61657381],\n",
      "        [ 0.34051073, -0.45201734],\n",
      "        [ 0.61815917, -0.41938546],\n",
      "        [ 0.46652883, -0.09068456],\n",
      "        [ 0.58977503, -0.47032225],\n",
      "        [ 0.67908180, -0.79428732],\n",
      "        [ 0.29152367,  0.04725356],\n",
      "        [ 0.34163851, -0.59187460],\n",
      "        [ 0.51391894, -0.37755948],\n",
      "        [ 0.33399636, -0.51906419],\n",
      "        [ 0.66016144, -0.53503853],\n",
      "        [ 0.68492448, -0.64312041],\n",
      "        [ 0.43182757, -0.54830915],\n",
      "        [ 0.49610978, -0.37509143],\n",
      "        [ 0.20831126, -0.73910475],\n",
      "        [ 0.70396292, -1.03938913],\n",
      "        [ 0.37078658,  0.22259869],\n",
      "        [ 0.62020016, -0.22986028],\n",
      "        [ 0.47899354, -0.53198564],\n",
      "        [ 0.61969984, -0.62715149],\n",
      "        [ 0.22881494, -0.59157366],\n",
      "        [ 0.23246156, -0.32783672],\n",
      "        [ 0.52364838, -0.29208496],\n",
      "        [ 0.52316898, -0.51109934],\n",
      "        [ 0.20134866, -0.47869277],\n",
      "        [ 0.35673782, -0.34941500],\n",
      "        [ 0.48148108, -0.48399442],\n",
      "        [ 0.62082517, -0.48972285],\n",
      "        [ 0.38070956, -0.29908007],\n",
      "        [ 0.32378298, -0.33146951],\n",
      "        [ 0.39163551, -0.70215625],\n",
      "        [ 0.47027287, -0.36038223],\n",
      "        [ 0.52589500, -0.31846553],\n",
      "        [ 0.20636782, -0.15001446],\n",
      "        [ 0.55274957, -0.31927019],\n",
      "        [ 0.67128491, -0.42472357],\n",
      "        [ 0.48520398, -0.41348913],\n",
      "        [ 0.55651456, -0.81761223],\n",
      "        [ 0.78031862, -0.18307100],\n",
      "        [ 0.31209758, -0.45207646],\n",
      "        [ 0.28851262, -0.70556927],\n",
      "        [ 0.90261877, -0.51536429],\n",
      "        [ 0.75212532, -0.31458282],\n",
      "        [ 0.66605759, -0.42316893],\n",
      "        [ 0.81974280, -0.20346040],\n",
      "        [ 0.50249273,  0.01756721],\n",
      "        [ 0.29340571, -0.28211784],\n",
      "        [ 0.60469878, -0.43947729],\n",
      "        [ 0.57227743, -0.40484762],\n",
      "        [ 0.54996222, -0.48694241],\n",
      "        [ 0.43367833, -0.51844543],\n",
      "        [ 0.85609138, -0.29044533]])\n",
      "loss: Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       0.60342896)\n",
      "acc: 0.65625\n",
      "global_step: 4\n",
      "step: 5\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 2900, 2255, ..., 1770, 117 , 102 ],\n",
      "        [101 , 2644, 1962, ..., 117 , 2340, 102 ],\n",
      "        [101 , 117 , 3680, ..., 4638, 4696, 102 ],\n",
      "        ...,\n",
      "        [101 , 872 , 1962, ..., 5168, 4916, 102 ],\n",
      "        [101 , 679 , 749 , ..., 686 , 4518, 102 ],\n",
      "        [101 , 2644, 1962, ..., 1355, 4873, 102 ]])\n",
      "logits: Tensor(shape=[64, 2], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.13337238, -0.39397585],\n",
      "        [ 0.59359729, -0.75730395],\n",
      "        [ 0.33812669, -0.62638521],\n",
      "        [ 0.54273260, -0.36298600],\n",
      "        [ 0.57121927, -0.84727985],\n",
      "        [ 0.41369000, -0.44954228],\n",
      "        [ 0.47174877, -0.22181955],\n",
      "        [ 0.05006765, -0.15527649],\n",
      "        [ 0.72074807, -0.63217390],\n",
      "        [ 0.40641934, -0.75276411],\n",
      "        [ 0.34154636, -0.37079692],\n",
      "        [ 0.53953367, -0.01135475],\n",
      "        [ 0.57458580, -0.34995407],\n",
      "        [ 0.20986950, -0.24074516],\n",
      "        [ 0.15477164, -0.10083160],\n",
      "        [ 0.71453166, -0.23075847],\n",
      "        [ 0.34661555, -0.59001273],\n",
      "        [ 0.51895791, -0.17068963],\n",
      "        [ 0.68739760, -0.63098717],\n",
      "        [ 0.45658273, -0.78803146],\n",
      "        [ 0.06805585, -0.79188704],\n",
      "        [ 0.56903398, -0.43014765],\n",
      "        [ 0.55454385, -0.31533292],\n",
      "        [ 0.40947706, -0.45145774],\n",
      "        [ 0.67928404, -0.70724249],\n",
      "        [ 0.46365348, -0.09517694],\n",
      "        [ 0.65015662, -0.57324004],\n",
      "        [ 0.61785889, -0.26011297],\n",
      "        [ 0.14105359, -0.39056116],\n",
      "        [ 0.47432002, -0.35529399],\n",
      "        [ 0.35876602, -0.34133640],\n",
      "        [ 0.77515459, -0.17540263],\n",
      "        [ 0.15472460, -0.20514405],\n",
      "        [ 0.55030811, -0.40145522],\n",
      "        [ 0.55164409, -0.38477278],\n",
      "        [ 0.47359484, -0.18557702],\n",
      "        [ 0.42029536, -0.36068779],\n",
      "        [ 0.69754136, -0.31374031],\n",
      "        [ 0.23319213, -0.12142310],\n",
      "        [ 0.67631841, -0.38074827],\n",
      "        [ 0.59398496, -0.07519737],\n",
      "        [ 0.63658738, -0.54302907],\n",
      "        [ 0.62927973, -0.80532157],\n",
      "        [ 0.62572730, -0.51291770],\n",
      "        [ 0.10377733, -0.65985799],\n",
      "        [ 0.62354743, -0.39036667],\n",
      "        [ 0.98127466, -0.66280437],\n",
      "        [ 0.19458859, -0.39867041],\n",
      "        [ 0.09251739, -0.52266312],\n",
      "        [ 0.26650369, -0.38068071],\n",
      "        [ 0.29468969, -0.66900849],\n",
      "        [ 0.31177169, -0.16099983],\n",
      "        [-0.09222901, -0.40719998],\n",
      "        [ 0.73320329, -0.57528228],\n",
      "        [ 0.57304764, -0.46843472],\n",
      "        [ 0.54050869, -0.70558321],\n",
      "        [ 0.61211240, -0.51524222],\n",
      "        [ 0.57280004,  0.02734880],\n",
      "        [ 0.32571101, -0.10177246],\n",
      "        [ 0.50178432, -1.12484026],\n",
      "        [ 0.29363537, -0.46926335],\n",
      "        [ 0.33382693, -0.44884115],\n",
      "        [ 0.59520060, -0.67180479],\n",
      "        [ 0.64551461, -0.52209240]])\n",
      "loss: Tensor(shape=[], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       0.58782744)\n",
      "acc: 0.6625\n",
      "global_step: 5\n",
      "step: 6\n",
      "input_ids: Tensor(shape=[64, 128], dtype=int32, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[101 , 3299, 7218, ..., 2552, 117 , 102 ],\n",
      "        [101 , 117 , 3680, ..., 4638, 4696, 102 ],\n",
      "        [101 , 2428, 117 , ..., 4542, 5991, 102 ],\n",
      "        ...,\n",
      "        [101 , 117 , 2160, ..., 4289, 1736, 102 ],\n",
      "        [101 , 4802, 4638, ..., 2553, 7557, 102 ],\n",
      "        [101 , 1898, 7509, ..., 4385, 1762, 102 ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "global_step = 0\n",
    "with LogWriter(logdir=\"./log\") as writer:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"epoch:\",epoch)\n",
    "        for step, batch in enumerate(train_loader, start=1): #从训练数据迭代器中取数据\n",
    "            print(\"step:\",step)\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            print(\"input_ids:\",input_ids)\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            print(\"logits:\",logits)\n",
    "            loss = criterion(logits, labels) #计算损失\n",
    "            print(\"loss:\",loss)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "            print(\"acc:\",acc)\n",
    "\n",
    "            global_step += 1\n",
    "            print('global_step:',global_step)\n",
    "            if global_step % 50 == 0 :\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "                #记录训练过程\n",
    "                writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "                writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_gradients()\n",
    "        eval_loss, eval_acc = evaluate(model, criterion, metric, dev_loader)\n",
    "        #eval_loss2, eval_acc2 = evaluate(model, criterion, metric, dev_loader2)\n",
    "        # 在每个epoch后保存模型\n",
    "        paddle.save(model.state_dict(), f'./saved_models/model_epoch{epoch}.pdparams')\n",
    "        #记录评估过程\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\n",
    "        writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)\n",
    "        #writer.add_scalar(tag=\"eval/loss2\", step=epoch, value=eval_loss2)\n",
    "        #writer.add_scalar(tag=\"eval/acc2\", step=epoch, value=eval_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:08:23.067617Z",
     "start_time": "2024-10-17T08:08:21.150124Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2024-10-17 16:08:21,155] [    INFO]\u001B[0m - Already cached C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 16:08:21,157] [    INFO]\u001B[0m - Loading weights file model_state.pdparams from cache at C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 16:08:22,026] [    INFO]\u001B[0m - Loaded weights file from disk, setting weights to model.\u001B[0m\n",
      "\u001B[33m[2024-10-17 16:08:23,013] [ WARNING]\u001B[0m - Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001B[0m\n",
      "\u001B[33m[2024-10-17 16:08:23,014] [ WARNING]\u001B[0m - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "#用于加载训练模型权重验证\n",
    "model2 = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#保存模型\n",
    "paddle.save(model.state_dict(), f'./saved_models/model_final.pdparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=4>可以看到，使用BERT预训练模型进行finetune，在第2个epoch后验证集准确率已经达到99.4%以上，在第3个epoch就能达到99.6%以上。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、预测效果\n",
    "\n",
    "完成上面的模型训练之后，可以得到一个能够通过中文邮件内容识别是否为垃圾邮件的模型。接下来查看模型在测试集上的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:08:06.987788Z",
     "start_time": "2024-10-17T08:08:06.972785Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(text, tokenizer, label_list=label_map.values(),  max_seq_length=128, is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id)): fn(samples)\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data = ['您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺']\n",
    "label_map = {0: '垃圾邮件', 1: '正常邮件'}\n",
    "data1=translate_text(\"It doesn't look like there's much interest in a bof;maybe next time.\")\n",
    "data2=translate_text(\"if you want a bof, schedule it and see what happens.i'm a little curious myself.\")\n",
    "data=['您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺',data1,data2]\n",
    "predictions = predict(model, data, tokenizer, label_map, batch_size=32)\n",
    "for idx, text in enumerate(data):\n",
    "    print('预测内容: {} \\n邮件标签: {}'.format(text, predictions[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:08:58.872664Z",
     "start_time": "2024-10-17T08:08:48.434840Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n",
      "D:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:1865: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测内容: 老师好，请问一下这周需要上课吗 \n",
      "邮件标签: 正常邮件\n",
      "预测内容: 看起来对bof不太感兴趣；也许下次吧。 \n",
      "邮件标签: 正常邮件\n",
      "预测内容: 如果你想要一个bof ，安排它，看看会发生什么。我自己也有点好奇。 \n",
      "邮件标签: 正常邮件\n"
     ]
    }
   ],
   "source": [
    "params_path = './saved_models/model_epoch1.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model2.set_state_dict(state_dict)\n",
    "data = ['您好我公司有多余的发票可以向外代开,国税,地税,运输,广告,海关缴款书如果贵公司,厂,有需要请来电洽谈,咨询联系电话,罗先生谢谢顺祝商祺']\n",
    "label_map = {0: '垃圾邮件', 1: '正常邮件'}\n",
    "data1=translate_text(\"It doesn't look like there's much interest in a bof;maybe next time.\")\n",
    "data2=translate_text(\"if you want a bof, schedule it and see what happens.i'm a little curious myself.\")\n",
    "data=['老师好，请问一下这周需要上课吗',data1,data2]\n",
    "predictions = predict(model2, data, tokenizer, label_map, batch_size=32)\n",
    "for idx, text in enumerate(data):\n",
    "    print('预测内容: {} \\n邮件标签: {}'.format(text, predictions[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:15:52.303906Z",
     "start_time": "2024-10-17T08:15:48.408587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\nlp_email\\.venv\\lib\\site-packages (3.9.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\nlp_email\\.venv\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\nlp_email\\.venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\nlp_email\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\nlp_email\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\nlp_email\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\nlp_email\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T08:17:26.110934Z",
     "start_time": "2024-10-17T08:16:00.325424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2024-10-17 16:16:01,395] [    INFO]\u001B[0m - Already cached C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 16:16:01,396] [    INFO]\u001B[0m - Loading weights file model_state.pdparams from cache at C:\\Users\\biang\\.paddlenlp\\models\\bert-base-chinese\\model_state.pdparams\u001B[0m\n",
      "\u001B[32m[2024-10-17 16:16:02,069] [    INFO]\u001B[0m - Loaded weights file from disk, setting weights to model.\u001B[0m\n",
      "\u001B[33m[2024-10-17 16:16:02,748] [ WARNING]\u001B[0m - Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001B[0m\n",
      "\u001B[33m[2024-10-17 16:16:02,751] [ WARNING]\u001B[0m - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n",
      "D:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\tokenizer_utils_base.py:2293: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 100\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader, start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     99\u001B[0m     input_ids, segment_ids, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m--> 100\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegment_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(logits, labels)  \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n\u001B[0;32m    102\u001B[0m     probs \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(logits, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\NLP_email\\.venv\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1254\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1246\u001B[0m     (\u001B[38;5;129;01mnot\u001B[39;00m in_declarative_mode())\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks)\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1251\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m in_profiler_mode())\n\u001B[0;32m   1252\u001B[0m ):\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_once(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dygraph_call_func(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\bert\\modeling.py:706\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, attention_mask, labels, output_hidden_states, output_attentions, return_dict)\u001B[0m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    655\u001B[0m \u001B[38;5;124;03mThe BertForSequenceClassification forward method, overrides the __call__() special method.\u001B[39;00m\n\u001B[0;32m    656\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    702\u001B[0m \n\u001B[0;32m    703\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    704\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 706\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    711\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    712\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    715\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    717\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[1;32mD:\\NLP_email\\.venv\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1254\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1246\u001B[0m     (\u001B[38;5;129;01mnot\u001B[39;00m in_declarative_mode())\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks)\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1251\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m in_profiler_mode())\n\u001B[0;32m   1252\u001B[0m ):\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_once(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dygraph_call_func(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\NLP_email\\.venv\\lib\\site-packages\\paddlenlp\\transformers\\bert\\modeling.py:427\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, attention_mask, past_key_values, use_cache, output_hidden_states, output_attentions, return_dict)\u001B[0m\n\u001B[0;32m    424\u001B[0m     past_key_values_length \u001B[38;5;241m=\u001B[39m past_key_values[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    426\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m paddle\u001B[38;5;241m.\u001B[39munsqueeze(\n\u001B[1;32m--> 427\u001B[0m         (\u001B[43minput_ids\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler\u001B[38;5;241m.\u001B[39mdense\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1e4\u001B[39m, axis\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m    428\u001B[0m     )\n\u001B[0;32m    429\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    430\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m past_key_values[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#增加新的评估参数\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import DataLoader\n",
    "from paddle.metric import Accuracy\n",
    "from visualdl import LogWriter\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 加载预训练模型Bert用于文本分类任务的Fine-tune网络BertForSequenceClassification\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)\n",
    "\n",
    "# 设置训练超参数\n",
    "learning_rate = 1e-6\n",
    "epochs = 1\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "num_warmup_steps = int(warmup_proportion * num_training_steps)\n",
    "\n",
    "def get_lr_factor(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    else:\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "\n",
    "# 学习率调度器\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\n",
    "\n",
    "# 优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# 评估函数\n",
    "metric = Accuracy()\n",
    "\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        \n",
    "        # 获取预测结果\n",
    "        preds = F.softmax(logits, axis=1).argmax(axis=1).numpy()\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        \n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "\n",
    "    accu = metric.accumulate()\n",
    "    \n",
    "    # 计算精确率和召回率\n",
    "    precision = metrics.precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = metrics.recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = metrics.f1_score(all_labels, all_preds, average='binary')\n",
    "    \n",
    "    print(f\"eval loss: {np.mean(losses):.5f}, accu: {accu:.5f}, precision: {precision:.5f}, recall: {recall:.5f}, f1: {f1:.5f}\")\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    confusion_mtx = metrics.confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(confusion_mtx)\n",
    "    \n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu \n",
    "\n",
    "def plot_confusion_matrix(confusion_mtx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# 开始训练\n",
    "global_step = 0\n",
    "with LogWriter(logdir=\"./log\") as writer:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            loss = criterion(logits, labels)  # 计算损失\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 50 == 0:\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "                # 记录训练过程\n",
    "                writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "                writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_gradients()\n",
    "\n",
    "        eval_loss, eval_acc = evaluate(model, criterion, metric, dev_loader)\n",
    "        paddle.save(model.state_dict(), f'./saved_models/model_epoch{epoch}.pdparams')\n",
    "        # 记录评估过程\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\n",
    "        writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
